<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1.0"
    />
    <title>Edge-Ready NLP Summarization | Efficient Transformers</title>
    <link rel="stylesheet" href="styles.css" />
    <link
      rel="preconnect"
      href="https://cdn.jsdelivr.net"
      crossorigin
    />
    <script
      src="https://cdn.jsdelivr.net/npm/chart.js@4.4.6/dist/chart.umd.min.js"
      defer
    ></script>
    <script type="module" src="app.js" defer></script>
  </head>
  <body>
    <header class="hero">
      <div class="hero-content">
        <p class="eyebrow">Low-Latency On-Device NLP</p>
        <h1>
          Efficient Transformer Summarization distilled on SST-2 for the edge
        </h1>
        <p>
          Distill, prune, and quantize a transformer student to deliver real-time summarization on
          Raspberry Pi, Jetson Nano, and Android—without cloud calls.
        </p>
        <div class="hero-actions">
          <a href="#demo" class="btn primary">Run On-Device Demo</a>
          <a href="#methodology" class="btn ghost">Pipeline Details</a>
        </div>
        <div class="hero-metrics">
          <div>
            <span class="metric-label">Latency (INT8 Jetson)</span>
            <span class="metric-value">21 ms</span>
          </div>
          <div>
            <span class="metric-label">Accuracy Δ vs Teacher</span>
            <span class="metric-value">-0.9%</span>
          </div>
          <div>
            <span class="metric-label">Power Draw</span>
            <span class="metric-value">3.1 W</span>
          </div>
        </div>
      </div>
    </header>

    <main>
      <section id="overview" class="panel">
        <h2>Problem Statement</h2>
        <p>
          Modern transformers excel at NLP, yet their compute footprint makes on-device deployment
          difficult. This project develops an edge-ready summarization stack that ingests SST-2
          sentences, distills knowledge from a large teacher, applies structured pruning and
          quantization-aware training, and exports to ONNX/TensorRT for latency-critical platforms.
        </p>
        <div class="grid">
          <article>
            <h3>Objectives</h3>
            <ul>
              <li>Sub-40 ms latency on Cortex-A78 & Jetson Nano</li>
              <li>&lt; 4 W peak power with battery-friendly duty cycles</li>
              <li>&gt; 93% sentiment-aware summarization accuracy vs SST-2 gold labels</li>
            </ul>
          </article>
          <article>
            <h3>Key Features</h3>
            <ul>
              <li>Memory budget under 400 MB</li>
              <li>Noise-robust summarization for SMS/chat text</li>
              <li>Edge-first pipeline: distill → prune → quantize → deploy</li>
            </ul>
          </article>
        </div>
      </section>

      <section id="methodology" class="panel">
        <h2>Methodology</h2>
        <div class="timeline">
          <article>
            <span class="step">01</span>
            <h3>Teacher → Student Distillation</h3>
            <p>
              Start with BART-large fine-tuned on SST-2 rationales. Train a 6-layer student via
              knowledge distillation combining cross-entropy, KL divergence, and attention transfer.
            </p>
          </article>
          <article>
            <span class="step">02</span>
            <h3>Structured Pruning</h3>
            <p>
              Remove low-importance attention heads and shrink intermediate FFN width using
              movement-pruning. Maintain accuracy with layer-wise adaptive re-weighting.
            </p>
          </article>
          <article>
            <span class="step">03</span>
            <h3>Quantization-Aware Training</h3>
            <p>
              Apply 8-bit activation / 4-bit weight QAT to match mobile DSP formats. Calibrate on
              SST-2 minibatches with injected noise to mimic field data.
            </p>
          </article>
          <article>
            <span class="step">04</span>
            <h3>ONNX & TensorRT Export</h3>
            <p>
              Convert the quantized student to ONNX Runtime (CPU/GPU) and TensorRT (Jetson). Fuse
              attention kernels and enable INT8 calibration for deterministic latency.
            </p>
          </article>
        </div>
      </section>

      <section id="datasets" class="panel">
        <h2>SST-2 Driven Summarization</h2>
        <p>
          Although SST-2 is a sentiment dataset, each example provides rich contextual cues. We
          harness these sentences as summarization prompts to teach the student model how to compress
          opinionated text while preserving sentiment polarity.
        </p>
        <div class="grid dataset-grid">
          <article>
            <h3>Dataset Snapshot</h3>
            <ul id="dataset-stats" class="stats"></ul>
          </article>
          <article>
            <h3>Sample Sentences</h3>
            <div id="sample-list" class="samples"></div>
          </article>
        </div>
      </section>

      <section id="experiments" class="panel">
        <h2>Latency / Accuracy Studies</h2>
        <div class="grid">
          <article>
            <canvas id="latencyChart" width="400" height="260"></canvas>
            <p class="chart-caption">
              Quantization delivers a 3&times; latency drop while keeping SST-2 summarization accuracy
              within 1 percentage point of the teacher.
            </p>
          </article>
          <article>
            <canvas id="energyChart" width="400" height="260"></canvas>
            <p class="chart-caption">
              Edge accelerators maintain sub-4 W envelope enabling always-on summarization on battery
              constrained gateways.
            </p>
          </article>
        </div>
      </section>

      <section id="demo" class="panel highlight">
        <div class="demo-header">
          <div>
            <h2>Browser-Based Edge Demo</h2>
            <p>
              Everything on this page runs locally: we load SST-2 samples, pass them through a distilled
              summarizer (`Xenova/distilbart-cnn-6-6`), and measure latency directly in your browser.
            </p>
          </div>
          <button id="rerun-demo" class="btn secondary">Rerun Summarizer</button>
        </div>
        <div class="demo-grid">
          <article>
            <h3>Input</h3>
            <textarea id="demo-input" rows="5"></textarea>
          </article>
          <article>
            <h3>Summary</h3>
            <div id="demo-output" class="output-panel">
              <p class="placeholder">Loading quantized model…</p>
            </div>
            <ul id="demo-metrics" class="stats compact"></ul>
          </article>
        </div>
      </section>

      <section id="deployment" class="panel">
        <h2>Deployment Blueprint</h2>
        <div class="grid">
          <article>
            <h3>Edge Stack</h3>
            <ul>
              <li>ONNX Runtime (CPU/GPU/DSP)</li>
              <li>TensorRT INT8 (Jetson Nano, Xavier NX)</li>
              <li>Android NNAPI + Vulkan backend</li>
              <li>WebAssembly demo</li>
            </ul>
          </article>
          <article>
            <h3>Best Practices</h3>
            <ul>
              <li>Profile attention bottlenecks with sparsity-aware kernels</li>
              <li>Fuse layernorm + matmul to maximize cache locality</li>
              <li>Use dynamic batching for gateway deployments</li>
              <li>Quantize embeddings separately for stability</li>
            </ul>
          </article>
        </div>
      </section>
    </main>

    <footer>
      <p>
        Built with distilled transformers, QAT, and SST-2 cues for resilient on-device NLP.
        Everything executes locally for a true edge experience.
      </p>
    </footer>
  </body>
</html>

